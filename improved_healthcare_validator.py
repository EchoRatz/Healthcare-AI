#!/usr/bin/env python3
"""
Improved Healthcare Validator - Reduce "à¸‡" Answers
==================================================

Smart Thai healthcare policy logic to avoid over-conservative "None" answers
"""

import re
import logging
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class HealthcareValidationResult:
    """Result from healthcare validation"""
    original_answer: List[str]
    validated_answer: List[str]
    confidence: float
    corrections_made: List[str]
    reasoning: str

class ImprovedHealthcareValidator:
    """Smart Thai healthcare validator that reduces excessive 'à¸‡' answers"""
    
    def __init__(self):
        # Thai healthcare policy knowledge base
        self.healthcare_policies = {
            "à¸ªà¸´à¸—à¸˜à¸´à¸«à¸¥à¸±à¸à¸›à¸£à¸°à¸à¸±à¸™à¸ªà¸¸à¸‚à¸ à¸²à¸žà¹à¸«à¹ˆà¸‡à¸Šà¸²à¸•à¸´": {
                "includes": [
                    "à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸£à¸±à¸à¸©à¸²à¸žà¸¢à¸²à¸šà¸²à¸¥à¸—à¸±à¹ˆà¸§à¹„à¸›", "à¸¢à¸²à¸ˆà¸³à¹€à¸›à¹‡à¸™", "à¸à¸²à¸£à¸œà¹ˆà¸²à¸•à¸±à¸”", "à¸à¸²à¸£à¸Ÿà¸·à¹‰à¸™à¸Ÿà¸¹",
                    "à¸à¸²à¸£à¸£à¸±à¸à¸©à¸²à¹‚à¸£à¸„à¹€à¸£à¸·à¹‰à¸­à¸£à¸±à¸‡", "à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ªà¸¸à¸‚à¸ à¸²à¸ž", "à¸§à¸±à¸„à¸‹à¸µà¸™", "à¸à¸²à¸£à¸„à¸¥à¸­à¸”"
                ],
                "excludes": [
                    "à¸à¸²à¸£à¸£à¸±à¸à¸©à¸²à¹€à¸ªà¸£à¸´à¸¡à¸„à¸§à¸²à¸¡à¸‡à¸²à¸¡", "à¸¢à¸²à¹à¸šà¸£à¸™à¸”à¹Œà¹€à¸™à¸¡", "à¸„à¹ˆà¸²à¸«à¹‰à¸­à¸‡à¸žà¸´à¹€à¸¨à¸©", 
                    "à¸à¸²à¸£à¸£à¸±à¸à¸©à¸²à¸—à¸”à¸¥à¸­à¸‡", "à¸­à¸¸à¸›à¸à¸£à¸“à¹Œà¹€à¸ªà¸£à¸´à¸¡", "à¸à¸²à¸£à¸—à¹ˆà¸­à¸‡à¹€à¸—à¸µà¹ˆà¸¢à¸§à¹€à¸žà¸·à¹ˆà¸­à¸ªà¸¸à¸‚à¸ à¸²à¸ž"
                ],
                "keywords": ["à¸«à¸¥à¸±à¸à¸›à¸£à¸°à¸à¸±à¸™", "à¸ªà¸¸à¸‚à¸ à¸²à¸žà¹à¸«à¹ˆà¸‡à¸Šà¸²à¸•à¸´", "UC", "30à¸šà¸²à¸—"]
            },
            "à¸ªà¸´à¸—à¸˜à¸´à¸šà¸±à¸•à¸£à¸—à¸­à¸‡": {
                "includes": [
                    "à¸à¸²à¸£à¸£à¸±à¸à¸©à¸²à¸Ÿà¸£à¸µ", "à¸¢à¸²à¸Ÿà¸£à¸µ", "à¸•à¸£à¸§à¸ˆà¸ªà¸¸à¸‚à¸ à¸²à¸žà¸›à¸£à¸°à¸ˆà¸³à¸›à¸µ", "à¸à¸²à¸£à¸”à¸¹à¹à¸¥à¸œà¸¹à¹‰à¸ªà¸¹à¸‡à¸­à¸²à¸¢à¸¸",
                    "à¸šà¸£à¸´à¸à¸²à¸£à¸—à¸µà¹ˆà¸šà¹‰à¸²à¸™", "à¸­à¸¸à¸›à¸à¸£à¸“à¹Œà¸à¸²à¸£à¹à¸žà¸—à¸¢à¹Œ", "à¸à¸²à¸£à¸Ÿà¸·à¹‰à¸™à¸Ÿà¸¹à¸ªà¸¸à¸‚à¸ à¸²à¸ž"
                ],
                "excludes": [
                    "à¸„à¹ˆà¸²à¹ƒà¸Šà¹‰à¸ˆà¹ˆà¸²à¸¢", "30à¸šà¸²à¸—", "à¹€à¸‡à¸´à¸™à¸ªà¸”", "à¸„à¹ˆà¸²à¸šà¸£à¸´à¸à¸²à¸£", "à¸„à¹ˆà¸²à¸•à¸£à¸§à¸ˆ"
                ],
                "keywords": ["à¸šà¸±à¸•à¸£à¸—à¸­à¸‡", "à¸œà¸¹à¹‰à¸ªà¸¹à¸‡à¸­à¸²à¸¢à¸¸", "à¸Ÿà¸£à¸µ", "60à¸›à¸µ"]
            },
            "à¸ªà¸´à¸—à¸˜à¸´30à¸šà¸²à¸—à¸£à¸±à¸à¸©à¸²à¸—à¸¸à¸à¹‚à¸£à¸„": {
                "includes": [
                    "à¸„à¹ˆà¸²à¸šà¸£à¸´à¸à¸²à¸£30à¸šà¸²à¸—", "à¸£à¸±à¸à¸©à¸²à¹‚à¸£à¸„à¸—à¸±à¹ˆà¸§à¹„à¸›", "à¸¢à¸²à¸ˆà¸³à¹€à¸›à¹‡à¸™", "à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸£à¸±à¸à¸©à¸²",
                    "à¸šà¸£à¸´à¸à¸²à¸£à¸œà¸¹à¹‰à¸›à¹ˆà¸§à¸¢à¸™à¸­à¸", "à¸à¸²à¸£à¸ªà¹ˆà¸‡à¸•à¹ˆà¸­"
                ],
                "excludes": [
                    "à¸Ÿà¸£à¸µ", "à¹„à¸¡à¹ˆà¹€à¸ªà¸µà¸¢à¸„à¹ˆà¸²à¹ƒà¸Šà¹‰à¸ˆà¹ˆà¸²à¸¢", "à¸šà¸±à¸•à¸£à¸—à¸­à¸‡", "à¸œà¸¹à¹‰à¸ªà¸¹à¸‡à¸­à¸²à¸¢à¸¸à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™"
                ],
                "keywords": ["30à¸šà¸²à¸—", "à¸£à¸±à¸à¸©à¸²à¸—à¸¸à¸à¹‚à¸£à¸„", "à¸„à¹ˆà¸²à¸šà¸£à¸´à¸à¸²à¸£", "UC"]
            }
        }
        
        # Question type patterns
        self.question_patterns = {
            "what_is_included": [
                r"à¸£à¸§à¸¡à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™", r"à¹„à¸”à¹‰à¸£à¸±à¸š", r"à¸¡à¸µà¸ªà¸´à¸—à¸˜à¸´à¹Œ", r"à¸„à¸£à¸­à¸šà¸„à¸¥à¸¸à¸¡", r"à¸›à¸£à¸°à¸à¸­à¸šà¸”à¹‰à¸§à¸¢"
            ],
            "what_is_excluded": [
                r"à¹„à¸¡à¹ˆà¸£à¸§à¸¡à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™", r"à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸£à¸±à¸š", r"à¹„à¸¡à¹ˆà¸¡à¸µà¸ªà¸´à¸—à¸˜à¸´à¹Œ", r"à¹„à¸¡à¹ˆà¸„à¸£à¸­à¸šà¸„à¸¥à¸¸à¸¡", r"à¸¢à¸à¹€à¸§à¹‰à¸™"
            ],
            "comparison": [
                r"à¹à¸•à¸à¸•à¹ˆà¸²à¸‡", r"à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š", r"à¸„à¸§à¸²à¸¡à¹à¸•à¸à¸•à¹ˆà¸²à¸‡", r"à¹€à¸«à¸¡à¸·à¸­à¸™", r"à¸•à¹ˆà¸²à¸‡"
            ],
            "qualification": [
                r"à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚", r"à¸„à¸¸à¸“à¸ªà¸¡à¸šà¸±à¸•à¸´", r"à¸ªà¸´à¸—à¸˜à¸´à¹Œ", r"à¹„à¸”à¹‰à¸£à¸±à¸šà¸ªà¸´à¸—à¸˜à¸´à¹Œ", r"à¸¡à¸µà¸ªà¸´à¸—à¸˜à¸´à¹Œ"
            ]
        }
        
        # Common wrong patterns that lead to too many "à¸‡" answers
        self.avoid_none_patterns = [
            "à¸¡à¸µà¸ªà¸´à¸—à¸˜à¸´à¹Œà¹„à¸”à¹‰à¸£à¸±à¸š",  # Usually has valid answers
            "à¸„à¸£à¸­à¸šà¸„à¸¥à¸¸à¸¡",      # Usually covers specific services
            "à¸›à¸£à¸°à¸à¸­à¸šà¸”à¹‰à¸§à¸¢",     # Usually lists specific items
            "à¸£à¸§à¸¡à¸–à¸¶à¸‡",         # Usually includes specific things
            "à¸à¸²à¸£à¸£à¸±à¸à¸©à¸²",       # Usually about specific treatments
        ]
    
    def validate_healthcare_answer(self, question: str, choices: Dict[str, str], predicted_answer: List[str]) -> HealthcareValidationResult:
        """Smart healthcare validation that reduces excessive 'à¸‡' answers"""
        
        logger.info(f"ðŸ¥ Healthcare validation: {predicted_answer}")
        
        original_answer = predicted_answer.copy()
        corrections_made = []
        reasoning_parts = []
        
        # Step 1: Detect question type
        question_type = self._detect_question_type(question)
        
        # Step 2: Check if "à¸‡" answer is actually justified
        if predicted_answer == ["à¸‡"]:
            better_answer = self._find_better_than_none(question, choices, question_type)
            if better_answer:
                predicted_answer = better_answer
                corrections_made.append("à¹à¸—à¸™à¸—à¸µà¹ˆ 'à¸‡' à¸”à¹‰à¸§à¸¢à¸„à¸³à¸•à¸­à¸šà¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸à¸§à¹ˆà¸²")
                reasoning_parts.append(f"à¸žà¸šà¸„à¸³à¸•à¸­à¸šà¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸à¸§à¹ˆà¸² 'à¹„à¸¡à¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¹ƒà¸”à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡'")
        
        # Step 3: Check for logical contradictions (but be less aggressive)
        if 'à¸‡' in predicted_answer and len(predicted_answer) > 1:
            # Only remove 'à¸‡' if other answers are clearly valid
            other_answers = [a for a in predicted_answer if a != 'à¸‡']
            if self._are_answers_clearly_valid(question, choices, other_answers):
                predicted_answer = other_answers
                corrections_made.append("à¸¥à¸š 'à¸‡' à¹€à¸žà¸£à¸²à¸°à¸¡à¸µà¸„à¸³à¸•à¸­à¸šà¸—à¸µà¹ˆà¸Šà¸±à¸”à¹€à¸ˆà¸™à¸­à¸·à¹ˆà¸™")
                reasoning_parts.append("à¸„à¸³à¸•à¸­à¸šà¸­à¸·à¹ˆà¸™à¸¡à¸µà¸„à¸§à¸²à¸¡à¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸Šà¸±à¸”à¹€à¸ˆà¸™")
        
        # Step 4: Apply healthcare policy knowledge
        policy_result = self._apply_policy_knowledge(question, choices, predicted_answer, question_type)
        if policy_result:
            predicted_answer = policy_result['answer']
            corrections_made.extend(policy_result['corrections'])
            reasoning_parts.append(policy_result['reasoning'])
        
        # Step 5: Calculate confidence (boost for non-"à¸‡" answers)
        confidence = self._calculate_smart_confidence(question, choices, predicted_answer, corrections_made)
        
        final_reasoning = " | ".join(reasoning_parts) if reasoning_parts else "à¹„à¸¡à¹ˆà¸¡à¸µà¸à¸²à¸£à¹à¸à¹‰à¹„à¸‚"
        
        if predicted_answer != original_answer:
            logger.info(f"âœ… Healthcare fix: {original_answer} â†’ {predicted_answer}")
        
        return HealthcareValidationResult(
            original_answer=original_answer,
            validated_answer=predicted_answer,
            confidence=confidence,
            corrections_made=corrections_made,
            reasoning=final_reasoning
        )
    
    def _detect_question_type(self, question: str) -> str:
        """Detect the type of healthcare question"""
        for q_type, patterns in self.question_patterns.items():
            for pattern in patterns:
                if re.search(pattern, question):
                    return q_type
        return "general"
    
    def _find_better_than_none(self, question: str, choices: Dict[str, str], question_type: str) -> Optional[List[str]]:
        """Find better answers than 'à¸‡' (None) based on healthcare knowledge"""
        
        # Check if this question pattern usually has valid answers
        has_avoid_none_pattern = any(pattern in question for pattern in self.avoid_none_patterns)
        
        if not has_avoid_none_pattern:
            return None  # Keep "à¸‡" for genuinely negative questions
        
        # Score each choice based on healthcare policy relevance
        choice_scores = {}
        
        for choice_key, choice_text in choices.items():
            score = 0
            
            # Score based on question type and policy knowledge
            if question_type == "what_is_included":
                score += self._score_inclusion_relevance(question, choice_text)
            elif question_type == "what_is_excluded":
                score += self._score_exclusion_relevance(question, choice_text)
            elif question_type == "qualification":
                score += self._score_qualification_relevance(question, choice_text)
            else:
                score += self._score_general_relevance(question, choice_text)
            
            choice_scores[choice_key] = score
        
        # Find best scoring choices
        max_score = max(choice_scores.values()) if choice_scores else 0
        
        if max_score > 3:  # Threshold for confidence
            best_choices = [choice for choice, score in choice_scores.items() 
                          if score >= max_score * 0.8]  # Within 80% of max score
            
            # Limit to reasonable number of choices
            if len(best_choices) <= 2:
                return sorted(best_choices)  # Sort for consistency
        
        return None
    
    def _score_inclusion_relevance(self, question: str, choice_text: str) -> int:
        """Score how relevant a choice is for inclusion questions"""
        score = 0
        
        # Check against healthcare policy includes
        for policy_name, policy_data in self.healthcare_policies.items():
            if any(keyword in question for keyword in policy_data['keywords']):
                # This question is about this specific policy
                for include_item in policy_data['includes']:
                    if include_item in choice_text or any(word in choice_text for word in include_item.split()):
                        score += 5
                
                # Penalty for excluded items
                for exclude_item in policy_data['excludes']:
                    if exclude_item in choice_text:
                        score -= 3
        
        # General healthcare inclusion terms
        inclusion_terms = ['à¸à¸²à¸£à¸£à¸±à¸à¸©à¸²', 'à¸šà¸£à¸´à¸à¸²à¸£', 'à¸¢à¸²', 'à¸•à¸£à¸§à¸ˆ', 'à¸”à¸¹à¹à¸¥', 'à¸Ÿà¸·à¹‰à¸™à¸Ÿà¸¹']
        for term in inclusion_terms:
            if term in choice_text:
                score += 2
        
        return score
    
    def _score_exclusion_relevance(self, question: str, choice_text: str) -> int:
        """Score how relevant a choice is for exclusion questions"""
        score = 0
        
        # For exclusion questions, look for things that are actually excluded
        for policy_name, policy_data in self.healthcare_policies.items():
            if any(keyword in question for keyword in policy_data['keywords']):
                # This question is about this specific policy
                for exclude_item in policy_data['excludes']:
                    if exclude_item in choice_text or any(word in choice_text for word in exclude_item.split()):
                        score += 5
                
                # Penalty for included items (they shouldn't be the answer for exclusion questions)
                for include_item in policy_data['includes']:
                    if include_item in choice_text:
                        score -= 2
        
        return score
    
    def _score_qualification_relevance(self, question: str, choice_text: str) -> int:
        """Score how relevant a choice is for qualification questions"""
        score = 0
        
        # Look for qualification-related terms
        qualification_terms = ['à¸­à¸²à¸¢à¸¸', 'à¸›à¸µ', 'à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚', 'à¸„à¸¸à¸“à¸ªà¸¡à¸šà¸±à¸•à¸´', 'à¸ªà¸´à¸—à¸˜à¸´à¹Œ', 'à¸¥à¸‡à¸—à¸°à¹€à¸šà¸µà¸¢à¸™']
        for term in qualification_terms:
            if term in choice_text:
                score += 3
        
        return score
    
    def _score_general_relevance(self, question: str, choice_text: str) -> int:
        """Score general relevance for other question types"""
        score = 0
        
        # Extract key terms from question
        question_terms = re.findall(r'[\u0E00-\u0E7F]+', question)
        choice_terms = re.findall(r'[\u0E00-\u0E7F]+', choice_text)
        
        # Score based on term overlap
        common_terms = set(question_terms) & set(choice_terms)
        score += len(common_terms)
        
        return score
    
    def _are_answers_clearly_valid(self, question: str, choices: Dict[str, str], answers: List[str]) -> bool:
        """Check if the given answers are clearly valid for the question"""
        if not answers:
            return False
        
        # Check if answers have good relevance scores
        total_score = 0
        question_type = self._detect_question_type(question)
        
        for answer in answers:
            choice_text = choices.get(answer, '')
            if question_type == "what_is_included":
                total_score += self._score_inclusion_relevance(question, choice_text)
            elif question_type == "what_is_excluded":
                total_score += self._score_exclusion_relevance(question, choice_text)
            else:
                total_score += self._score_general_relevance(question, choice_text)
        
        avg_score = total_score / len(answers)
        return avg_score > 2
    
    def _apply_policy_knowledge(self, question: str, choices: Dict[str, str], predicted_answer: List[str], question_type: str) -> Optional[Dict]:
        """Apply specific Thai healthcare policy knowledge"""
        
        # Look for specific policy conflicts
        corrections = []
        reasoning = ""
        
        # Check for à¸šà¸±à¸•à¸£à¸—à¸­à¸‡ vs 30à¸šà¸²à¸— conflict
        if question_type == "what_is_excluded":
            choice_texts = [choices.get(choice, '') for choice in predicted_answer]
            choice_text_combined = ' '.join(choice_texts)
            
            # If question is about à¸šà¸±à¸•à¸£à¸—à¸­à¸‡ and answer mentions 30à¸šà¸²à¸—, that's likely wrong
            if 'à¸šà¸±à¸•à¸£à¸—à¸­à¸‡' in question and '30à¸šà¸²à¸—' in choice_text_combined:
                # Remove choices mentioning 30à¸šà¸²à¸— for à¸šà¸±à¸•à¸£à¸—à¸­à¸‡ questions
                filtered_answers = []
                for choice in predicted_answer:
                    choice_text = choices.get(choice, '')
                    if '30à¸šà¸²à¸—' not in choice_text:
                        filtered_answers.append(choice)
                
                if filtered_answers != predicted_answer:
                    return {
                        'answer': filtered_answers or ['à¸‡'],  # Fallback to à¸‡ if nothing left
                        'corrections': ['à¸¥à¸šà¸„à¸³à¸•à¸­à¸šà¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š30à¸šà¸²à¸—à¸­à¸­à¸à¸ˆà¸²à¸à¸„à¸³à¸–à¸²à¸¡à¸šà¸±à¸•à¸£à¸—à¸­à¸‡'],
                        'reasoning': 'à¸šà¸±à¸•à¸£à¸—à¸­à¸‡à¹„à¸¡à¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š30à¸šà¸²à¸—'
                    }
        
        return None
    
    def _calculate_smart_confidence(self, question: str, choices: Dict[str, str], answer: List[str], corrections: List[str]) -> float:
        """Calculate confidence with bias against excessive 'à¸‡' answers"""
        base_confidence = 0.6
        
        # Boost confidence for non-"à¸‡" answers
        if answer != ["à¸‡"]:
            base_confidence += 0.15
        
        # Boost confidence for corrections that avoid "à¸‡"
        if corrections and any('à¹à¸—à¸™à¸—à¸µà¹ˆ' in correction for correction in corrections):
            base_confidence += 0.1
        
        # Boost for multiple choice answers (often more accurate than single "à¸‡")
        if len(answer) > 1 and 'à¸‡' not in answer:
            base_confidence += 0.05
        
        # Adjust based on question complexity
        if len(question) > 100:
            base_confidence -= 0.05
        
        return min(max(base_confidence, 0.1), 0.95)

def test_improved_validator():
    """Test the improved healthcare validator"""
    print("ðŸ§ª Testing Improved Healthcare Validator")
    print("=" * 45)
    
    validator = ImprovedHealthcareValidator()
    
    # Test cases that should NOT be "à¸‡"
    test_cases = [
        {
            "name": "Inclusion question (should have answers, not à¸‡)",
            "question": "à¸ªà¸´à¸—à¸˜à¸´à¸›à¸£à¸°à¸à¸±à¸™à¸ªà¸¸à¸‚à¸ à¸²à¸žà¹à¸«à¹ˆà¸‡à¸Šà¸²à¸•à¸´à¸„à¸£à¸­à¸šà¸„à¸¥à¸¸à¸¡à¸à¸²à¸£à¸£à¸±à¸à¸©à¸²à¹ƒà¸”à¸šà¹‰à¸²à¸‡?",
            "choices": {
                "à¸": "à¸à¸²à¸£à¸£à¸±à¸à¸©à¸²à¸žà¸¢à¸²à¸šà¸²à¸¥à¸—à¸±à¹ˆà¸§à¹„à¸›",
                "à¸‚": "à¸à¸²à¸£à¸£à¸±à¸à¸©à¸²à¹€à¸ªà¸£à¸´à¸¡à¸„à¸§à¸²à¸¡à¸‡à¸²à¸¡", 
                "à¸„": "à¸¢à¸²à¸ˆà¸³à¹€à¸›à¹‡à¸™",
                "à¸‡": "à¹„à¸¡à¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¹ƒà¸”à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡"
            },
            "predicted": ["à¸‡"],  # Over-conservative
            "should_improve": True
        },
        {
            "name": "à¸šà¸±à¸•à¸£à¸—à¸­à¸‡ benefits (should have answers)",
            "question": "à¸œà¸¹à¹‰à¸–à¸·à¸­à¸šà¸±à¸•à¸£à¸—à¸­à¸‡à¹„à¸”à¹‰à¸£à¸±à¸šà¸ªà¸´à¸—à¸˜à¸´à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œà¸­à¸°à¹„à¸£à¸šà¹‰à¸²à¸‡?",
            "choices": {
                "à¸": "à¸à¸²à¸£à¸£à¸±à¸à¸©à¸²à¸Ÿà¸£à¸µ",
                "à¸‚": "à¸„à¹ˆà¸²à¸šà¸£à¸´à¸à¸²à¸£30à¸šà¸²à¸—",
                "à¸„": "à¸•à¸£à¸§à¸ˆà¸ªà¸¸à¸‚à¸ à¸²à¸žà¸›à¸£à¸°à¸ˆà¸³à¸›à¸µ", 
                "à¸‡": "à¹„à¸¡à¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¹ƒà¸”à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡"
            },
            "predicted": ["à¸‡"],  # Over-conservative
            "should_improve": True
        },
        {
            "name": "Valid exclusion question (à¸‡ might be correct)",
            "question": "à¸ªà¸´à¸—à¸˜à¸´à¹ƒà¸™à¸‚à¹‰à¸­à¹ƒà¸”à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸£à¸§à¸¡à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸ªà¸´à¸—à¸˜à¸´à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œà¸‚à¸­à¸‡à¸œà¸¹à¹‰à¸¡à¸µà¸ªà¸´à¸—à¸˜à¸´à¸«à¸¥à¸±à¸à¸›à¸£à¸°à¸à¸±à¸™à¸ªà¸¸à¸‚à¸ à¸²à¸žà¹à¸«à¹ˆà¸‡à¸Šà¸²à¸•à¸´?",
            "choices": {
                "à¸": "à¸ªà¸´à¸—à¸˜à¸´à¸«à¸¥à¸±à¸à¸›à¸£à¸°à¸à¸±à¸™à¸ªà¸¸à¸‚à¸ à¸²à¸žà¹à¸«à¹ˆà¸‡à¸Šà¸²à¸•à¸´",
                "à¸‚": "à¸ªà¸´à¸—à¸˜à¸´à¸šà¸±à¸•à¸£à¸—à¸­à¸‡",
                "à¸„": "à¸ªà¸´à¸—à¸˜à¸´ 30 à¸šà¸²à¸—à¸£à¸±à¸à¸©à¸²à¸—à¸¸à¸à¹‚à¸£à¸„",
                "à¸‡": "à¹„à¸¡à¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¹ƒà¸”à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡"
            },
            "predicted": ["à¸‡"],  # This might actually be correct
            "should_improve": False
        }
    ]
    
    improvements = 0
    
    for test_case in test_cases:
        print(f"\nðŸ“‹ Test: {test_case['name']}")
        print(f"   Question: {test_case['question'][:50]}...")
        print(f"   Original: {test_case['predicted']}")
        
        result = validator.validate_healthcare_answer(
            test_case['question'],
            test_case['choices'],
            test_case['predicted']
        )
        
        print(f"   Validated: {result.validated_answer}")
        print(f"   Confidence: {result.confidence:.2f}")
        
        if result.corrections_made:
            print(f"   âœ… Corrections: {result.corrections_made}")
            print(f"   ðŸ“ Reasoning: {result.reasoning}")
            
            if test_case['should_improve'] and result.validated_answer != ["à¸‡"]:
                improvements += 1
                print(f"   ðŸŽ¯ IMPROVED! Avoided excessive 'à¸‡'")
            elif not test_case['should_improve']:
                print(f"   ðŸ“ Appropriately kept 'à¸‡' for exclusion question")
        else:
            print(f"   ðŸ“ No changes made")
    
    print(f"\nðŸ“Š Results:")
    print(f"  Improvements: {improvements}/{sum(1 for tc in test_cases if tc['should_improve'])}")
    print(f"  This validator should reduce excessive 'à¸‡' answers!")
    
    return improvements > 0

if __name__ == "__main__":
    test_improved_validator()