#!/usr/bin/env python3
"""
Test Optimized Healthcare Q&A System
====================================

Performance comparison and validation script
"""

import os
import sys
import time
import csv
from optimized_healthcare_qa_system import OptimizedHealthcareQA

def test_single_question():
    """Test single question processing"""
    print("üß™ Testing single question...")
    
    qa_system = OptimizedHealthcareQA()
    
    test_question = {
        'id': '1',
        'question': '‡∏ú‡∏°‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å ‡∏≠‡πâ‡∏ß‡∏Å‡∏î‡πâ‡∏ß‡∏¢ ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏ï‡∏µ‡∏™‡∏≠‡∏á‡∏¢‡∏±‡∏á‡∏°‡∏µ‡πÅ‡∏ú‡∏ô‡∏Å‡πÑ‡∏´‡∏ô‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö?  ‡∏Å. Endocrinology ‡∏Ç. Orthopedics ‡∏Ñ. Emergency ‡∏á. Internal Medicine'
    }
    
    # Parse question
    question, choices = qa_system.parse_question_fast(test_question['question'])
    print(f"Question: {question}")
    print(f"Choices: {choices}")
    
    # Analyze question
    analysis = qa_system.analyze_question_fast(question)
    print(f"Analysis: {analysis}")
    
    # Search context
    context = qa_system.search_context_fast(analysis)
    print(f"Context length: {len(context)} chars")
    
    # Query LLM
    if qa_system.check_llama31():
        result = qa_system.query_llama31_optimized(question, choices, context)
        print(f"Answer: {result.answer}")
        print(f"Confidence: {result.confidence}")
        print(f"Reasoning: {result.reasoning}")
    else:
        print("‚ùå Llama 3.1 not available")

def test_performance():
    """Test performance with small batch"""
    print("\n‚ö° Testing performance...")
    
    qa_system = OptimizedHealthcareQA()
    
    # Create small test file
    test_questions = [
        {'id': '1', 'question': '‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡πÉ‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏´‡∏•‡∏±‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥? ‡∏Å. ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏• ‡∏Ç. ‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î ‡∏Ñ. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ü‡∏±‡∏ô ‡∏á. ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û'},
        {'id': '2', 'question': '‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏Ñ‡∏•‡∏∑‡∏≠‡∏ö‡∏ü‡∏•‡∏π‡∏≠‡∏≠‡πÑ‡∏£‡∏î‡πå‡∏ä‡∏ô‡∏¥‡∏î‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏™‡∏π‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÄ‡∏´‡∏°‡∏≤‡∏à‡πà‡∏≤‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡πÉ‡∏î‡∏ï‡πà‡∏≠‡∏Ñ‡∏£‡∏±‡πâ‡∏á? ‡∏Å. 100 ‡∏ö‡∏≤‡∏ó ‡∏Ç. 200 ‡∏ö‡∏≤‡∏ó ‡∏Ñ. 300 ‡∏ö‡∏≤‡∏ó ‡∏á. 400 ‡∏ö‡∏≤‡∏ó'},
        {'id': '3', 'question': '‡∏Ç‡πâ‡∏≠‡πÉ‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô‡∏ß‡∏¥‡∏Å‡∏§‡∏ï‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πà‡∏≤‡∏¢‡∏™‡∏¥‡∏ó‡∏ò‡∏¥ UCEP? ‡∏Å. ‡∏õ‡∏ß‡∏î‡∏´‡∏±‡∏ß ‡∏Ç. ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏Ç‡πâ ‡∏Ñ. ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏ï‡πâ‡∏ô ‡∏á. ‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á'},
        {'id': '4', 'question': '‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏Ñ‡∏•‡∏≠‡∏î‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏´‡∏•‡∏±‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡πÉ‡∏î? ‡∏Å. ‡∏ó‡∏∏‡∏Å‡∏Å‡∏£‡∏ì‡∏µ ‡∏Ç. ‡∏Å‡∏£‡∏ì‡∏µ‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏Ñ. ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÅ‡∏û‡∏ó‡∏¢‡πå‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ ‡∏á. ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ'},
        {'id': '5', 'question': '‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà‡∏ü‡∏±‡∏ô‡∏õ‡∏•‡∏≠‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏≤‡∏¢‡∏∏‡πÄ‡∏ó‡πà‡∏≤‡πÉ‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ? ‡∏Å. 18 ‡∏õ‡∏µ ‡∏Ç. 20 ‡∏õ‡∏µ ‡∏Ñ. 25 ‡∏õ‡∏µ ‡∏á. 30 ‡∏õ‡∏µ'}
    ]
    
    # Save to temporary file
    temp_file = "temp_test.csv"
    with open(temp_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=['id', 'question'])
        writer.writeheader()
        for q in test_questions:
            writer.writerow(q)
    
    try:
        start_time = time.time()
        results = qa_system.process_questions_optimized(temp_file)
        end_time = time.time()
        
        print(f"‚è±Ô∏è  Processing time: {end_time - start_time:.2f} seconds")
        print(f"üìä Questions per second: {len(results)/(end_time - start_time):.2f}")
        
        for result in results:
            print(f"  Q{result['id']}: {result['answer']} (confidence: {result['confidence']:.2f})")
            
    finally:
        # Clean up
        if os.path.exists(temp_file):
            os.remove(temp_file)

def test_single_choice_enforcement():
    """Test that only single choices are returned"""
    print("\n‚úÖ Testing single-choice enforcement...")
    
    qa_system = OptimizedHealthcareQA()
    
    # Test cases that might generate multiple answers
    test_cases = [
        "‡∏Ç‡πâ‡∏≠‡πÉ‡∏î‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏´‡∏•‡∏±‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û? ‡∏Å. ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤ ‡∏Ç. ‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ü‡∏±‡∏ô ‡∏Ñ. ‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î ‡∏á. ‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡∏¢‡∏≤",
        "‡πÅ‡∏ú‡∏ô‡∏Å‡πÉ‡∏î‡πÄ‡∏õ‡∏¥‡∏î‡∏ï‡∏•‡∏≠‡∏î 24 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á? ‡∏Å. ‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô ‡∏Ç. ‡∏®‡∏±‡∏•‡∏¢‡∏Å‡∏£‡∏£‡∏° ‡∏Ñ. ‡∏≠‡∏≤‡∏¢‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏° ‡∏á. ‡∏Å‡∏∏‡∏°‡∏≤‡∏£‡πÄ‡∏ß‡∏ä‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå",
        "‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÉ‡∏î‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡πÉ‡∏ô‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏´‡∏•‡∏±‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô? ‡∏Å. ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à ‡∏Ç. ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤ ‡∏Ñ. ‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î ‡∏á. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ü‡∏±‡∏ô"
    ]
    
    for i, question_text in enumerate(test_cases, 1):
        question, choices = qa_system.parse_question_fast(question_text)
        analysis = qa_system.analyze_question_fast(question)
        context = qa_system.search_context_fast(analysis)
        
        if qa_system.check_llama31():
            result = qa_system.query_llama31_optimized(question, choices, context)
            print(f"  Q{i}: Answer = '{result.answer}' (length: {len(result.answer)})")
            
            # Verify single choice
            if len(result.answer) == 1 and result.answer in ['‡∏Å', '‡∏Ç', '‡∏Ñ', '‡∏á']:
                print(f"    ‚úÖ Single choice enforced")
            else:
                print(f"    ‚ùå Multiple choices detected: {result.answer}")

async def compare_with_original():
    """Compare performance with original system"""
    print("\nüìä Performance comparison...")
    
    # Check if original system exists
    try:
        from improved_healthcare_qa_system import ImprovedHealthcareQA
        original_available = True
    except ImportError:
        original_available = False
        print("‚ö†Ô∏è  Original system not available for comparison")
    
    if not original_available:
        return
    
    # Create small test file for comparison
    test_questions = [
        {'id': '1', 'question': '‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡πÉ‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏´‡∏•‡∏±‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥? ‡∏Å. ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏• ‡∏Ç. ‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î ‡∏Ñ. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ü‡∏±‡∏ô ‡∏á. ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û'},
        {'id': '2', 'question': '‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏Ñ‡∏•‡∏∑‡∏≠‡∏ö‡∏ü‡∏•‡∏π‡∏≠‡∏≠‡πÑ‡∏£‡∏î‡πå‡∏ä‡∏ô‡∏¥‡∏î‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡πâ‡∏ô‡∏™‡∏π‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÄ‡∏´‡∏°‡∏≤‡∏à‡πà‡∏≤‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡πÉ‡∏î‡∏ï‡πà‡∏≠‡∏Ñ‡∏£‡∏±‡πâ‡∏á? ‡∏Å. 100 ‡∏ö‡∏≤‡∏ó ‡∏Ç. 200 ‡∏ö‡∏≤‡∏ó ‡∏Ñ. 300 ‡∏ö‡∏≤‡∏ó ‡∏á. 400 ‡∏ö‡∏≤‡∏ó'}
    ]
    
    temp_file = "temp_comparison.csv"
    with open(temp_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=['id', 'question'])
        writer.writeheader()
        for q in test_questions:
            writer.writerow(q)
    
    try:
        # Test optimized system
        print("Testing optimized system...")
        optimized_system = OptimizedHealthcareQA()
        start_time = time.time()
        optimized_results = optimized_system.process_questions_optimized(temp_file)
        optimized_time = time.time() - start_time
        
        # Test original system
        print("Testing original system...")
        original_system = ImprovedHealthcareQA()
        start_time = time.time()
        original_results = await original_system.process_questions_enhanced(temp_file)
        original_time = time.time() - start_time
        
        print(f"\nüìà Performance Comparison:")
        print(f"  Optimized system: {optimized_time:.2f}s ({len(optimized_results)/optimized_time:.2f} q/s)")
        print(f"  Original system: {original_time:.2f}s ({len(original_results)/original_time:.2f} q/s)")
        print(f"  Speed improvement: {original_time/optimized_time:.1f}x faster")
        
        # Check single-choice enforcement
        optimized_single = sum(1 for r in optimized_results if len(r['answer']) == 1)
        original_single = sum(1 for r in original_results if len(r['answer']) == 1)
        
        print(f"\n‚úÖ Single-choice enforcement:")
        print(f"  Optimized: {optimized_single}/{len(optimized_results)} single choices")
        print(f"  Original: {original_single}/{len(original_results)} single choices")
        
    finally:
        if os.path.exists(temp_file):
            os.remove(temp_file)

async def main():
    """Main test function"""
    print("üè• Optimized Healthcare Q&A System - Performance Tests")
    print("=" * 60)
    
    # Run tests
    test_single_question()
    test_performance()
    test_single_choice_enforcement()
    
    # Only run comparison if original system is available
    try:
        await compare_with_original()
    except Exception as e:
        print(f"‚ö†Ô∏è  Comparison test skipped: {e}")
    
    print("\nüéâ All tests completed!")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main()) 