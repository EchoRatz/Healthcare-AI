#!/usr/bin/env python3
"""
High-Accuracy Healthcare Q&A System for Llama 3.1 70B
=====================================================

Optimized system designed to achieve 75%+ accuracy with fast runtime.
Key improvements:
1. Advanced question understanding with intent classification
2. Semantic knowledge base indexing and retrieval
3. Optimized prompting for Llama 3.1 70B
4. Smart answer validation with policy awareness
5. Confidence-based answer selection
6. Fast processing with efficient algorithms
"""

import os
import sys
import csv
import json
import requests
import time
import re
import asyncio
from typing import Dict, List, Tuple, Optional, Set
from dataclasses import dataclass
from collections import defaultdict, Counter
import numpy as np

@dataclass
class QuestionIntent:
    """Detailed question intent analysis"""
    primary_type: str  # 'inclusion', 'exclusion', 'factual', 'procedure', 'comparison', 'emergency'
    secondary_type: str  # Additional context
    keywords: List[str]
    entities: List[str]
    numbers: List[str]
    policy_terms: List[str]
    department_terms: List[str]
    confidence: float
    urgency_level: int  # 1-5 scale

@dataclass
class ContextMatch:
    """Context matching result"""
    content: str
    relevance_score: float
    source: str
    keywords_matched: List[str]
    policy_related: bool

@dataclass
class AnswerAnalysis:
    """Detailed answer analysis"""
    selected_answers: List[str]
    confidence: float
    reasoning: str
    policy_validation: bool
    context_support: float
    alternatives: List[str]
    should_reject: bool

class HighAccuracyHealthcareQA:
    """High-accuracy healthcare Q&A system optimized for Llama 3.1 70B"""

    def __init__(self):
        self.model_name = None
        self.knowledge_base = {}
        self.semantic_index = None
        self.vectorizer = None
        self.healthcare_policies = self._load_comprehensive_policies()
        self.question_patterns = self._load_advanced_patterns()
        self.department_mapping = self._load_department_mapping()
        self.emergency_keywords = self._load_emergency_keywords()
        self.number_patterns = self._load_number_patterns()
        self.cache = {}
        
    def _load_comprehensive_policies(self) -> Dict:
        """Load comprehensive Thai healthcare policy knowledge"""
        return {
            "‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏´‡∏•‡∏±‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥": {
                "includes": [
                    "‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ", "‡∏¢‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô", "‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î", "‡∏Å‡∏≤‡∏£‡∏ü‡∏∑‡πâ‡∏ô‡∏ü‡∏π",
                    "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ‡πÄ‡∏£‡∏∑‡πâ‡∏≠‡∏£‡∏±‡∏á", "‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û", "‡∏ß‡∏±‡∏Ñ‡∏ã‡∏µ‡∏ô", "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏≠‡∏î",
                    "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô", "‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠", "‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏ó‡∏≤‡∏á‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£",
                    "‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô", "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÉ‡∏ô‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•", "‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÉ‡∏ô",
                    "‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á", "‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ", "‡∏Å‡∏≤‡∏£‡∏ü‡∏∑‡πâ‡∏ô‡∏ü‡∏π‡∏™‡∏°‡∏£‡∏£‡∏ñ‡∏†‡∏≤‡∏û",
                    "‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏ú‡∏π‡πâ‡∏™‡∏π‡∏á‡∏≠‡∏≤‡∏¢‡∏∏", "‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡πÄ‡∏î‡πá‡∏Å", "‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏´‡∏ç‡∏¥‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏£‡∏£‡∏†‡πå",
                    "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ó‡∏±‡∏ô‡∏ï‡∏Å‡∏£‡∏£‡∏°", "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ‡πÑ‡∏ï", "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ‡∏´‡∏±‡∏ß‡πÉ‡∏à",
                    "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ‡∏°‡∏∞‡πÄ‡∏£‡πá‡∏á", "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô", "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏ô‡πÇ‡∏•‡∏´‡∏¥‡∏ï‡∏™‡∏π‡∏á"
                ],
                "excludes": [
                    "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡∏≤‡∏°", "‡∏¢‡∏≤‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå‡πÄ‡∏ô‡∏°", "‡∏Ñ‡πà‡∏≤‡∏´‡πâ‡∏≠‡∏á‡∏û‡∏¥‡πÄ‡∏®‡∏©",
                    "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ó‡∏î‡∏•‡∏≠‡∏á", "‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡πÄ‡∏™‡∏£‡∏¥‡∏°", "‡∏Å‡∏≤‡∏£‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û",
                    "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô", "‡∏¢‡∏≤‡πÄ‡∏™‡∏£‡∏¥‡∏°", "‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô",
                    "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÉ‡∏ô‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡πÄ‡∏≠‡∏Å‡∏ä‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö", "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÉ‡∏ô‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®"
                ],
                "keywords": ["‡∏´‡∏•‡∏±‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô", "‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥", "UC", "30‡∏ö‡∏≤‡∏ó", "‡∏™‡∏õ‡∏™‡∏ä", "1330"],
                "coverage": "universal",
                "cost": "30‡∏ö‡∏≤‡∏ó"
            },
            "‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏ö‡∏±‡∏ï‡∏£‡∏ó‡∏≠‡∏á": {
                "includes": [
                    "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ü‡∏£‡∏µ", "‡∏¢‡∏≤‡∏ü‡∏£‡∏µ", "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏õ‡∏µ", "‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏ú‡∏π‡πâ‡∏™‡∏π‡∏á‡∏≠‡∏≤‡∏¢‡∏∏",
                    "‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ö‡πâ‡∏≤‡∏ô", "‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå", "‡∏Å‡∏≤‡∏£‡∏ü‡∏∑‡πâ‡∏ô‡∏ü‡∏π‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û",
                    "‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏á", "‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ", "‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß",
                    "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ‡πÄ‡∏£‡∏∑‡πâ‡∏≠‡∏£‡∏±‡∏á", "‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏ú‡∏π‡πâ‡∏û‡∏¥‡∏Å‡∏≤‡∏£", "‡∏Å‡∏≤‡∏£‡∏ü‡∏∑‡πâ‡∏ô‡∏ü‡∏π‡∏™‡∏°‡∏£‡∏£‡∏ñ‡∏†‡∏≤‡∏û"
                ],
                "excludes": [
                    "‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢", "30‡∏ö‡∏≤‡∏ó", "‡πÄ‡∏á‡∏¥‡∏ô‡∏™‡∏î", "‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£", "‡∏Ñ‡πà‡∏≤‡∏ï‡∏£‡∏ß‡∏à"
                ],
                "keywords": ["‡∏ö‡∏±‡∏ï‡∏£‡∏ó‡∏≠‡∏á", "‡∏ú‡∏π‡πâ‡∏™‡∏π‡∏á‡∏≠‡∏≤‡∏¢‡∏∏", "‡∏ü‡∏£‡∏µ", "60‡∏õ‡∏µ", "‡∏ú‡∏π‡πâ‡∏û‡∏¥‡∏Å‡∏≤‡∏£", "D1"],
                "coverage": "elderly_disabled",
                "cost": "‡∏ü‡∏£‡∏µ"
            },
            "‡∏™‡∏¥‡∏ó‡∏ò‡∏¥30‡∏ö‡∏≤‡∏ó‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ó‡∏∏‡∏Å‡πÇ‡∏£‡∏Ñ": {
                "includes": [
                    "‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£30‡∏ö‡∏≤‡∏ó", "‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏£‡∏Ñ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ", "‡∏¢‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô", "‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏£‡∏±‡∏Å‡∏©‡∏≤",
                    "‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ô‡∏≠‡∏Å", "‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠", "‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô"
                ],
                "excludes": [
                    "‡∏ü‡∏£‡∏µ", "‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢", "‡∏ö‡∏±‡∏ï‡∏£‡∏ó‡∏≠‡∏á", "‡∏ú‡∏π‡πâ‡∏™‡∏π‡∏á‡∏≠‡∏≤‡∏¢‡∏∏‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"
                ],
                "keywords": ["30‡∏ö‡∏≤‡∏ó", "‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ó‡∏∏‡∏Å‡πÇ‡∏£‡∏Ñ", "‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£", "UC"],
                "coverage": "general",
                "cost": "30‡∏ö‡∏≤‡∏ó"
            },
            "‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏±‡∏á‡∏Ñ‡∏°": {
                "includes": [
                    "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•", "‡∏¢‡∏≤", "‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à", "‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î",
                    "‡∏Å‡∏≤‡∏£‡∏ü‡∏∑‡πâ‡∏ô‡∏ü‡∏π", "‡∏Å‡∏≤‡∏£‡∏î‡∏π‡πÅ‡∏•‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÉ‡∏ô", "‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏ó‡∏≤‡∏á‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£"
                ],
                "excludes": [
                    "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏á‡∏≤‡∏°", "‡∏¢‡∏≤‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå‡πÄ‡∏ô‡∏°", "‡∏Ñ‡πà‡∏≤‡∏´‡πâ‡∏≠‡∏á‡∏û‡∏¥‡πÄ‡∏®‡∏©"
                ],
                "keywords": ["‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏±‡∏á‡∏Ñ‡∏°", "‡∏Ç‡∏™‡∏°‡∏Å", "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£"],
                "coverage": "employed",
                "cost": "copay"
            }
        }
    
    def _load_advanced_patterns(self) -> Dict:
        """Load advanced patterns for question classification"""
        return {
            "inclusion": [
                r"‡∏£‡∏ß‡∏°‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô", r"‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö", r"‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå", r"‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°", r"‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢",
                r"‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á", r"‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á", r"‡∏ã‡∏∂‡πà‡∏á‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á", r"‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á", r"‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ",
                r"‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£", r"‡∏£‡∏±‡∏ö‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£", r"‡πÉ‡∏ä‡πâ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥", r"‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"
            ],
            "exclusion": [
                r"‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô", r"‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö", r"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå", r"‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°", r"‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô",
                r"‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà", r"‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ç‡πâ‡∏≠‡πÉ‡∏î", r"‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á", r"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ", r"‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£"
            ],
            "factual": [
                r"‡πÄ‡∏ó‡πà‡∏≤‡πÉ‡∏î", r"‡∏Å‡∏µ‡πà‡∏ö‡∏≤‡∏ó", r"‡∏Å‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á", r"‡∏Å‡∏µ‡πà‡∏õ‡∏µ", r"‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏î",
                r"‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô", r"‡πÉ‡∏Ñ‡∏£", r"‡∏≠‡∏∞‡πÑ‡∏£", r"‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£", r"‡∏≠‡∏±‡∏ï‡∏£‡∏≤", r"‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢",
                r"‡∏£‡∏≤‡∏Ñ‡∏≤", r"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô", r"‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì", r"‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤"
            ],
            "procedure": [
                r"‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô", r"‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£", r"‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£", r"‡∏Ñ‡∏ß‡∏£‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£",
                r"‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ", r"‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ", r"‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç", r"‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£", r"‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£"
            ],
            "comparison": [
                r"‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á", r"‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö", r"‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á", r"‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô", r"‡∏ï‡πà‡∏≤‡∏á",
                r"‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤", r"‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤", r"‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö", r"‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤", r"‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤"
            ],
            "emergency": [
                r"‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô", r"‡∏ß‡∏¥‡∏Å‡∏§‡∏ï", r"‡πÄ‡∏à‡πá‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏Å", r"‡∏´‡∏≤‡∏¢‡πÉ‡∏à‡∏•‡∏≥‡∏ö‡∏≤‡∏Å", r"‡∏´‡∏°‡∏î‡∏™‡∏ï‡∏¥",
                r"‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡∏≠‡∏≠‡∏Å", r"‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏", r"‡πÄ‡∏à‡πá‡∏ö‡∏õ‡πà‡∏ß‡∏¢‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô", r"UCEP"
            ],
            "department": [
                r"‡πÅ‡∏ú‡∏ô‡∏Å", r"department", r"‡πÅ‡∏ú‡∏ô‡∏Å‡πÑ‡∏´‡∏ô", r"‡πÅ‡∏ú‡∏ô‡∏Å‡πÉ‡∏î", r"‡∏û‡∏ö‡∏´‡∏°‡∏≠",
                r"‡∏™‡πà‡∏á‡πÅ‡∏ú‡∏ô‡∏Å", r"‡πÅ‡∏ú‡∏ô‡∏Å‡πÇ‡∏£‡∏Ñ", r"‡πÅ‡∏ú‡∏ô‡∏Å‡∏≠‡∏≠‡∏£‡πå‡πÇ‡∏ò‡∏õ‡∏¥‡∏î‡∏¥‡∏Å‡∏™‡πå", r"‡πÅ‡∏ú‡∏ô‡∏Å‡πÇ‡∏£‡∏Ñ‡∏´‡∏±‡∏ß‡πÉ‡∏à"
            ]
        }
    
    def _load_department_mapping(self) -> Dict:
        """Load department mapping for better classification"""
        return {
            "cardiology": ["‡πÇ‡∏£‡∏Ñ‡∏´‡∏±‡∏ß‡πÉ‡∏à", "cardiology", "‡∏´‡∏±‡∏ß‡πÉ‡∏à", "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏ô", "‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏ß‡∏≤‡∏¢"],
            "orthopedics": ["‡∏≠‡∏≠‡∏£‡πå‡πÇ‡∏ò‡∏õ‡∏¥‡∏î‡∏¥‡∏Å‡∏™‡πå", "orthopedics", "‡∏Å‡∏£‡∏∞‡∏î‡∏π‡∏Å", "‡∏Ç‡πâ‡∏≠", "‡∏õ‡∏ß‡∏î‡∏´‡∏•‡∏±‡∏á", "spine"],
            "emergency": ["‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô", "emergency", "ER", "‡∏ß‡∏¥‡∏Å‡∏§‡∏ï", "UCEP"],
            "neurology": ["‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó", "neurology", "‡∏™‡∏°‡∏≠‡∏á", "‡πÄ‡∏™‡πâ‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó"],
            "endocrinology": ["‡∏ï‡πà‡∏≠‡∏°‡πÑ‡∏£‡πâ‡∏ó‡πà‡∏≠", "endocrinology", "‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô", "‡∏Æ‡∏≠‡∏£‡πå‡πÇ‡∏°‡∏ô"],
            "internal_medicine": ["‡∏≠‡∏≤‡∏¢‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°", "internal medicine", "‡∏≠‡∏≤‡∏¢‡∏∏‡∏£"],
            "psychiatry": ["‡∏à‡∏¥‡∏ï‡πÄ‡∏ß‡∏ä", "psychiatry", "‡∏à‡∏¥‡∏ï", "‡∏ß‡∏¥‡∏ï‡∏Å‡∏Å‡∏±‡∏á‡∏ß‡∏•"],
            "nephrology": ["‡πÑ‡∏ï", "nephrology", "‡πÑ‡∏ï‡∏ß‡∏≤‡∏¢", "‡∏•‡πâ‡∏≤‡∏á‡πÑ‡∏ï"],
            "dental": ["‡∏ó‡∏±‡∏ô‡∏ï‡∏Å‡∏£‡∏£‡∏°", "dental", "‡∏ü‡∏±‡∏ô", "‡∏ó‡∏±‡∏ô‡∏ï"],
            "pediatrics": ["‡∏Å‡∏∏‡∏°‡∏≤‡∏£", "pediatrics", "‡πÄ‡∏î‡πá‡∏Å", "‡∏ó‡∏≤‡∏£‡∏Å"]
        }
    
    def _load_emergency_keywords(self) -> Set:
        """Load emergency-related keywords"""
        return {
            "‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô", "‡∏ß‡∏¥‡∏Å‡∏§‡∏ï", "‡πÄ‡∏à‡πá‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏Å", "‡∏´‡∏≤‡∏¢‡πÉ‡∏à‡∏•‡∏≥‡∏ö‡∏≤‡∏Å", "‡∏´‡∏°‡∏î‡∏™‡∏ï‡∏¥", "‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡∏≠‡∏≠‡∏Å",
            "‡∏≠‡∏∏‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏´‡∏ï‡∏∏", "‡πÄ‡∏à‡πá‡∏ö‡∏õ‡πà‡∏ß‡∏¢‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô", "UCEP", "‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏ß‡∏≤‡∏¢", "stroke",
            "emergency", "critical", "chest pain", "shortness of breath"
        }
    
    def _load_number_patterns(self) -> Dict:
        """Load patterns for number extraction"""
        return {
            "money": [r"(\d+(?:,\d+)*)\s*‡∏ö‡∏≤‡∏ó", r"(\d+)\s*‡∏ö‡∏≤‡∏ó/‡πÄ‡∏°‡πá‡∏î", r"(\d+)\s*‡∏ö‡∏≤‡∏ó/‡∏Ñ‡∏£‡∏±‡πâ‡∏á"],
            "age": [r"‡∏≠‡∏≤‡∏¢‡∏∏\s*(\d+)\s*‡∏õ‡∏µ", r"(\d+)\s*‡∏õ‡∏µ"],
            "time": [r"(\d{1,2}):(\d{2})", r"(\d+)\s*‡πÇ‡∏°‡∏á", r"(\d+)\s*‡∏ô‡∏≤‡∏ó‡∏µ"],
            "quantity": [r"(\d+)\s*‡∏Ñ‡∏£‡∏±‡πâ‡∏á", r"(\d+)\s*‡∏Ç‡∏ß‡∏î", r"(\d+)\s*‡∏≠‡∏±‡∏ô"],
            "year": [r"‡∏õ‡∏µ\s*(\d{4})", r"‡∏û\.‡∏®\.\s*(\d{4})", r"‡∏Ñ\.‡∏®\.\s*(\d{4})"]
        } 
    
    def check_llama31(self) -> bool:
        """Check for Llama 3.1 availability"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=3)
            if response.status_code == 200:
                models = response.json().get("models", [])
                for model in models:
                    if "llama3.1" in model["name"].lower():
                        self.model_name = model["name"]
                        return True
            return False
        except:
            return False
    
    def load_knowledge_base(self):
        """Load and create semantic index of knowledge base"""
        print("üìö Loading and indexing knowledge base...")
        
        doc_files = [
            "Healthcare-AI-Refactored/src/infrastructure/results_doc/direct_extraction_corrected.txt",
            "Healthcare-AI-Refactored/src/infrastructure/results_doc2/direct_extraction_corrected.txt",
            "Healthcare-AI-Refactored/src/infrastructure/results_doc3/direct_extraction_corrected.txt",
        ]
        
        all_content = []
        content_sections = []
        
        for i, doc_file in enumerate(doc_files, 1):
            if os.path.exists(doc_file):
                with open(doc_file, "r", encoding="utf-8") as f:
                    content = f.read()
                    sections = self._split_into_sections(content)
                    
                    for j, section in enumerate(sections):
                        if len(section.strip()) > 100:  # Only meaningful sections
                            section_id = f"doc_{i}_section_{j}"
                            content_sections.append({
                                'id': section_id,
                                'content': section,
                                'doc_id': f"doc_{i}",
                                'keywords': self._extract_keywords_advanced(section)
                            })
                            all_content.append(section)
                    
                    print(f"  ‚úÖ Document {i}: {len(sections)} sections indexed")
            else:
                print(f"  ‚ö†Ô∏è  Document {i} not found: {doc_file}")
        
        # Create simple semantic search (without sklearn dependency)
        if all_content:
            self.knowledge_sections = content_sections
            print(f"  üß† Knowledge base indexed with {len(all_content)} sections")
    
    def _split_into_sections(self, content: str) -> List[str]:
        """Split content into meaningful sections"""
        # Split by page markers
        sections = re.split(r'--- Page \d+ ---', content)
        
        # Further split by natural breaks
        final_sections = []
        for section in sections:
            if len(section.strip()) > 50:
                # Split by double newlines or Q/A patterns
                subsections = re.split(r'\n\n+', section)
                final_sections.extend([s.strip() for s in subsections if len(s.strip()) > 50])
        
        return final_sections
    
    def _extract_keywords_advanced(self, text: str) -> List[str]:
        """Extract relevant keywords with advanced techniques"""
        keywords = []
        
        # Thai healthcare specific keywords
        thai_keywords = [
            "‡∏™‡∏¥‡∏ó‡∏ò‡∏¥", "‡∏´‡∏•‡∏±‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô", "‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û", "‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤", "‡∏¢‡∏≤", "‡∏ï‡∏£‡∏ß‡∏à", "‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î",
            "‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•", "‡πÅ‡∏û‡∏ó‡∏¢‡πå", "‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢", "‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢", "‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£", "‡πÇ‡∏£‡∏Ñ",
            "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏≠‡∏î", "‡∏ß‡∏±‡∏Ñ‡∏ã‡∏µ‡∏ô", "‡∏â‡∏∏‡∏Å‡πÄ‡∏â‡∏¥‡∏ô", "‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠", "‡πÉ‡∏ö‡∏™‡πà‡∏á‡∏ï‡∏±‡∏ß", "‡∏ö‡∏±‡∏ï‡∏£‡∏ó‡∏≠‡∏á",
            "‡∏™‡∏õ‡∏™‡∏ä", "1330", "30‡∏ö‡∏≤‡∏ó", "‡∏ü‡∏£‡∏µ", "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏±‡∏á‡∏Ñ‡∏°", "‡∏Ç‡∏™‡∏°‡∏Å"
        ]
        
        # Extract policy terms
        for policy, info in self.healthcare_policies.items():
            if any(keyword in text for keyword in info["keywords"]):
                keywords.append(policy)
                keywords.extend(info["keywords"])
        
        # Extract department terms
        for dept, terms in self.department_mapping.items():
            if any(term in text for term in terms):
                keywords.append(dept)
                keywords.extend(terms)
        
        # Extract numbers
        for pattern_type, patterns in self.number_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, text)
                keywords.extend(matches)
        
        # Extract Thai words (2+ characters)
        thai_words = re.findall(r'[‡∏Å-‡∏Æ]{2,}', text)
        keywords.extend(thai_words[:10])  # Limit to top 10
        
        # Add specific keywords found in text
        for keyword in thai_keywords:
            if keyword in text:
                keywords.append(keyword)
        
        return list(set(keywords))  # Remove duplicates
    
    def analyze_question_advanced(self, question_text: str) -> QuestionIntent:
        """Advanced question analysis with multiple dimensions"""
        
        # Initialize analysis
        primary_type = "factual"
        secondary_type = "general"
        keywords = []
        entities = []
        numbers = []
        policy_terms = []
        department_terms = []
        confidence = 0.5
        urgency_level = 1
        
        # Detect question type with confidence scoring
        type_scores = defaultdict(float)
        for qtype, patterns in self.question_patterns.items():
            for pattern in patterns:
                if re.search(pattern, question_text, re.IGNORECASE):
                    type_scores[qtype] += 0.3
                    confidence += 0.1
        
        # Select primary type
        if type_scores:
            primary_type = max(type_scores, key=type_scores.get)
            if type_scores[primary_type] > 0.5:
                confidence += 0.2
        
        # Check for emergency
        if any(keyword in question_text.lower() for keyword in self.emergency_keywords):
            urgency_level = 5
            if primary_type == "factual":
                primary_type = "emergency"
        
        # Extract keywords and entities
        keywords = self._extract_keywords_advanced(question_text)
        
        # Extract policy terms
        for policy, info in self.healthcare_policies.items():
            if any(keyword in question_text for keyword in info["keywords"]):
                policy_terms.append(policy)
                entities.append(policy)
        
        # Extract department terms
        for dept, terms in self.department_mapping.items():
            if any(term in question_text.lower() for term in terms):
                department_terms.append(dept)
                entities.append(dept)
        
        # Extract numbers
        for pattern_type, patterns in self.number_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, question_text)
                numbers.extend(matches)
        
        # Determine secondary type
        if department_terms:
            secondary_type = "department"
        elif policy_terms:
            secondary_type = "policy"
        elif numbers:
            secondary_type = "numerical"
        
        return QuestionIntent(
            primary_type=primary_type,
            secondary_type=secondary_type,
            keywords=keywords,
            entities=entities,
            numbers=numbers,
            policy_terms=policy_terms,
            department_terms=department_terms,
            confidence=min(confidence, 1.0),
            urgency_level=urgency_level
        )
    
    def search_context_semantic(self, question_analysis: QuestionIntent, max_sections: int = 5) -> List[ContextMatch]:
        """Semantic search for relevant context using keyword matching"""
        if not hasattr(self, 'knowledge_sections'):
            return []
        
        # Score sections based on keyword overlap
        scored_sections = []
        for section in self.knowledge_sections:
            score = 0.0
            matched_keywords = []
            
            # Score based on keyword matches
            for keyword in question_analysis.keywords:
                if keyword in section['content']:
                    score += 0.2
                    matched_keywords.append(keyword)
            
            # Bonus for policy terms
            for policy in question_analysis.policy_terms:
                if policy in section['content']:
                    score += 0.3
            
            # Bonus for department terms
            for dept in question_analysis.department_terms:
                if dept in section['content']:
                    score += 0.2
            
            # Bonus for number matches
            for number in question_analysis.numbers:
                if number in section['content']:
                    score += 0.1
            
            if score > 0.1:  # Minimum relevance threshold
                scored_sections.append(ContextMatch(
                    content=section['content'],
                    relevance_score=score,
                    source=section['doc_id'],
                    keywords_matched=matched_keywords,
                    policy_related=any(policy in section['content'] for policy in question_analysis.policy_terms)
                ))
        
        # Sort by relevance and return top matches
        scored_sections.sort(key=lambda x: x.relevance_score, reverse=True)
        return scored_sections[:max_sections]
    
    def parse_question_enhanced(self, question_text: str) -> Tuple[str, Dict[str, str]]:
        """Enhanced question parsing with better choice extraction"""
        parts = question_text.split("‡∏Å.")
        if len(parts) < 2:
            return question_text, {}
        
        question = parts[0].strip()
        choices_text = "‡∏Å." + parts[1]
        
        # Enhanced choice extraction
        choice_pattern = re.compile(r"([‡∏Å-‡∏á])\.\s*([^‡∏Å-‡∏á]+?)(?=\s*[‡∏Å-‡∏á]\.|$)")
        choices = {}
        
        for match in choice_pattern.finditer(choices_text):
            choice_letter = match.group(1)
            choice_text = match.group(2).strip()
            choices[choice_letter] = choice_text
        
        return question, choices
    
    def build_optimized_prompt(self, question: str, choices: Dict[str, str], 
                             context_matches: List[ContextMatch], 
                             question_analysis: QuestionIntent) -> str:
        """Build optimized prompt for Llama 3.1 70B"""
        
        # Combine relevant context
        context_parts = []
        for match in context_matches[:3]:  # Top 3 most relevant
            if match.relevance_score > 0.2:
                context_parts.append(match.content)
        
        context = "\n\n".join(context_parts) if context_parts else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á"
        
        # Build structured prompt
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 20 ‡∏õ‡∏µ

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:
{context}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å:
"""
        
        for letter, text in choices.items():
            prompt += f"{letter}. {text}\n"
        
        prompt += f"""
‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question_analysis.primary_type}
‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: {', '.join(question_analysis.keywords[:5])}
‡πÄ‡∏≠‡∏ô‡∏ó‡∏¥‡∏ï‡∏µ‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {', '.join(question_analysis.entities[:3])}
‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô: {question_analysis.urgency_level}/5

‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
1. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
2. ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
3. ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
4. ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö "‡∏á"
5. ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡πÄ‡∏ä‡πà‡∏ô "‡∏Å" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏Ç,‡∏Ñ" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏á"
6. ‡∏≠‡∏¢‡πà‡∏≤‡∏ï‡∏≠‡∏ö "‡∏á" ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:"""
        
        return prompt
    
    def query_llama31_optimized(self, question: str, choices: Dict[str, str], 
                               context_matches: List[ContextMatch], 
                               question_analysis: QuestionIntent) -> Tuple[List[str], float]:
        """Optimized query to Llama 3.1 70B"""
        
        # Build optimized prompt
        prompt = self.build_optimized_prompt(question, choices, context_matches, question_analysis)
        
        try:
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,  # Very low temperature for consistency
                        "top_p": 0.9,
                        "top_k": 40,
                        "repeat_penalty": 1.1,
                        "num_predict": 50  # Limit response length
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                answer_text = result.get("response", "").strip()
                
                # Extract answers with enhanced parsing
                answers = self._extract_answers_optimized(answer_text, choices)
                confidence = self._calculate_confidence_advanced(answer_text, question_analysis, context_matches)
                
                return answers, confidence
            else:
                print(f"‚ùå API Error: {response.status_code}")
                return [], 0.0
                
        except Exception as e:
            print(f"‚ùå Query Error: {e}")
            return [], 0.0
    
    def _extract_answers_optimized(self, text: str, choices: Dict[str, str]) -> List[str]:
        """Optimized answer extraction with multiple fallback strategies"""
        
        # Multiple patterns for answer extraction
        patterns = [
            r"‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö[:\s]*([‡∏Å-‡∏á](?:[,\s]+[‡∏Å-‡∏á])*)",
            r"‡∏ï‡∏≠‡∏ö[:\s]*([‡∏Å-‡∏á](?:[,\s]+[‡∏Å-‡∏á])*)",
            r"([‡∏Å-‡∏á](?:[,\s]+[‡∏Å-‡∏á])*)",
            r"‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å[:\s]*([‡∏Å-‡∏á](?:[,\s]+[‡∏Å-‡∏á])*)",
            r"‡πÄ‡∏•‡∏∑‡∏≠‡∏Å[:\s]*([‡∏Å-‡∏á](?:[,\s]+[‡∏Å-‡∏á])*)"
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                answer_text = match.group(1)
                # Clean and validate
                answers = re.findall(r'[‡∏Å-‡∏á]', answer_text)
                if answers:
                    return list(set(answers))  # Remove duplicates
        
        # Fallback: look for any ‡∏Å-‡∏á in the text
        answers = re.findall(r'[‡∏Å-‡∏á]', text)
        if answers:
            return list(set(answers))
        
        return []
    
    def _calculate_confidence_advanced(self, answer_text: str, question_analysis: QuestionIntent, 
                                     context_matches: List[ContextMatch]) -> float:
        """Advanced confidence calculation"""
        confidence = 0.5  # Base confidence
        
        # Higher confidence if answer is clear
        if "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö" in answer_text or "‡∏ï‡∏≠‡∏ö" in answer_text:
            confidence += 0.2
        
        # Higher confidence if question analysis was successful
        confidence += question_analysis.confidence * 0.3
        
        # Higher confidence if we have relevant context
        if context_matches and any(match.relevance_score > 0.3 for match in context_matches):
            confidence += 0.2
        
        # Lower confidence if answer is "‡∏á" and we have good context
        if "‡∏á" in answer_text and context_matches and any(match.relevance_score > 0.4 for match in context_matches):
            confidence -= 0.1
        
        # Higher confidence for emergency questions
        if question_analysis.urgency_level >= 4:
            confidence += 0.1
        
        return min(max(confidence, 0.0), 1.0)
    
    def validate_answer_advanced(self, question: str, choices: Dict[str, str], 
                               answers: List[str], question_analysis: QuestionIntent) -> AnswerAnalysis:
        """Advanced answer validation with policy awareness"""
        
        if not answers:
            return AnswerAnalysis([], 0.0, "No answers found", False, 0.0, [], True)
        
        # Check for logical contradictions
        if "‡∏á" in answers and len(answers) > 1:
            return AnswerAnalysis(
                ["‡∏á"], 0.3, 
                "Contradiction: '‡∏á' cannot be combined with other answers",
                False, 0.3, ["‡∏á"], False
            )
        
        # Check for all choices selected
        if len(answers) >= 4 and all(c in answers for c in ['‡∏Å', '‡∏Ç', '‡∏Ñ', '‡∏á']):
            return AnswerAnalysis(
                ["‡∏á"], 0.2,
                "All choices selected including '‡∏á' - likely none are correct",
                False, 0.2, ["‡∏á"], False
            )
        
        # Validate against healthcare policies
        policy_validation = self._validate_against_policies_advanced(question, choices, answers, question_analysis)
        if not policy_validation:
            return AnswerAnalysis(
                answers, 0.4,
                "Policy validation failed",
                False, 0.4, answers, False
            )
        
        # Check for emergency context
        if question_analysis.urgency_level >= 4:
            # Emergency questions should have specific answers
            if "‡∏á" in answers and len(answers) == 1:
                return AnswerAnalysis(
                    answers, 0.3,
                    "Emergency question should have specific answer, not '‡∏á'",
                    True, 0.3, answers, True
                )
        
        return AnswerAnalysis(
            answers, 0.8, 
            "Answer validated successfully", 
            True, 0.8, answers, False
        )
    
    def _validate_against_policies_advanced(self, question: str, choices: Dict[str, str], 
                                          answers: List[str], question_analysis: QuestionIntent) -> bool:
        """Advanced policy validation"""
        
        # Check for policy-specific contradictions
        for policy, info in self.healthcare_policies.items():
            if any(keyword in question for keyword in info["keywords"]):
                # Check if answers contradict policy
                for answer in answers:
                    choice_text = choices.get(answer, "")
                    if any(exclude in choice_text for exclude in info["excludes"]):
                        return False
        
        # Check for department-specific logic
        if question_analysis.department_terms:
            # Department questions should have specific answers
            if "‡∏á" in answers and len(answers) == 1 and question_analysis.primary_type != "exclusion":
                return False
        
        return True
    
    def process_questions_high_accuracy(self, test_file: str) -> List[Dict]:
        """Process questions with high accuracy optimization"""

        if not self.check_llama31():
            print("‚ùå Llama 3.1 not available")
            return []

        print(f"‚úÖ Using model: {self.model_name}")

        # Load knowledge base
        self.load_knowledge_base()

        # Load test questions
        questions = []
        with open(test_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                questions.append(row)

        print(f"üöÄ Processing {len(questions)} questions with high accuracy optimization...")

        results = []
        start_time = time.time()

        for i, row in enumerate(questions, 1):
            question_id = row['id']
            question_text = row['question']

            # Parse question
            question, choices = self.parse_question_enhanced(question_text)

            # Analyze question
            question_analysis = self.analyze_question_advanced(question)

            # Search for context
            context_matches = self.search_context_semantic(question_analysis)

            # Query LLM
            answers, confidence = self.query_llama31_optimized(question, choices, context_matches, question_analysis)

            # Validate answer
            answer_analysis = self.validate_answer_advanced(question, choices, answers, question_analysis)

            # Apply final answer
            final_answers = answer_analysis.selected_answers if not answer_analysis.should_reject else answers

            # Format answer
            answer_str = ",".join(final_answers) if final_answers else "‡∏á"

            results.append({
                'id': question_id,
                'answer': answer_str,
                'confidence': confidence,
                'validation_passed': answer_analysis.policy_validation,
                'reasoning': answer_analysis.reasoning,
                'question_type': question_analysis.primary_type,
                'urgency_level': question_analysis.urgency_level,
                'context_relevance': max([match.relevance_score for match in context_matches]) if context_matches else 0.0
            })

            # Progress update
            if i % 25 == 0:
                elapsed = time.time() - start_time
                rate = i / elapsed
                eta = (len(questions) - i) / rate if rate > 0 else 0
                print(f"  üìä {i}/{len(questions)} ({i/len(questions)*100:.1f}%) | Rate: {rate:.1f} q/s | ETA: {eta/60:.1f}min")

        total_time = time.time() - start_time
        print(f"üéâ High accuracy processing complete!")
        print(f"‚è±Ô∏è  Total time: {total_time/60:.1f} minutes")

        return results
    
    def save_results(self, results: List[Dict], output_file: str):
        """Save results to CSV"""
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['id', 'answer'])
            writer.writeheader()
            for result in results:
                writer.writerow({
                    'id': result['id'],
                    'answer': result['answer']
                })
        
        print(f"üíæ Results saved to: {output_file}")

async def main():
    """Main function"""
    print("üè• HIGH-ACCURACY HEALTHCARE Q&A SYSTEM FOR LLAMA 3.1 70B")
    print("=" * 70)

    qa_system = HighAccuracyHealthcareQA()

    # Process questions
    test_file = "Healthcare-AI-Refactored/src/infrastructure/test.csv"
    results = await qa_system.process_questions_high_accuracy(test_file)

    if results:
        # Save results
        output_file = "high_accuracy_healthcare_submission.csv"
        qa_system.save_results(results, output_file)

        # Print detailed summary
        total_questions = len(results)
        high_confidence = sum(1 for r in results if r['confidence'] > 0.7)
        validation_passed = sum(1 for r in results if r['validation_passed'])
        emergency_questions = sum(1 for r in results if r['urgency_level'] >= 4)
        avg_confidence = sum(r['confidence'] for r in results) / total_questions
        avg_context_relevance = sum(r['context_relevance'] for r in results) / total_questions

        print(f"\nüìä DETAILED SUMMARY:")
        print(f"  Total questions: {total_questions}")
        print(f"  High confidence answers: {high_confidence} ({high_confidence/total_questions*100:.1f}%)")
        print(f"  Validation passed: {validation_passed} ({validation_passed/total_questions*100:.1f}%)")
        print(f"  Emergency questions: {emergency_questions}")
        print(f"  Average confidence: {avg_confidence:.3f}")
        print(f"  Average context relevance: {avg_context_relevance:.3f}")
        
        # Question type breakdown
        type_counts = Counter(r['question_type'] for r in results)
        print(f"\nüìã Question Type Breakdown:")
        for qtype, count in type_counts.most_common():
            print(f"  {qtype}: {count} ({count/total_questions*100:.1f}%)")
    else:
        print("‚ùå No results generated")

if __name__ == "__main__":
    asyncio.run(main()) 